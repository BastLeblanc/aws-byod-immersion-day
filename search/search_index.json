{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Data Engineering \u00b6 Welcome fellow analytics enthusiast. This is a support web-page for instructor led Labs around AWS data platform services. As of now we cover a few Data Engineering use-cases, such as hydrating the datalake, orchestration with Glue, data visualization and governance. Stay tuned, there\u2019s more to come.","title":"Data Engineering"},{"location":"index.html#data-engineering","text":"Welcome fellow analytics enthusiast. This is a support web-page for instructor led Labs around AWS data platform services. As of now we cover a few Data Engineering use-cases, such as hydrating the datalake, orchestration with Glue, data visualization and governance. Stay tuned, there\u2019s more to come.","title":"Data Engineering"},{"location":"01_ingestion_with_glue/ingestion_with_glue.html","text":"Introduction \u00b6 In this Lab we will create a schema from your data optimized for analytics and place the result in an S3 bucket based data lake. Before you begin \u00b6 Please make sure now you selected the region where your data resides. All resources to be created must to be in the same region. Preparing your environment \u00b6 Before you start, make sure your raw data files are saved in a separate bucket in a folder called \u201craw\u201d. Each file should be a separate table. Each table file should be preferably in a separate folder with the table name. An example would be as follows: <schema_name>/<table_name>/LOAD00000001.csv <schema_name>/<table_name>/LOAD00000002.csv ... <schema_name>/<table_name>/LOAD00000009.csv <schema_name>/<table_name>/LOAD0000000A.csv <schema_name>/<table_name>/LOAD0000000B.csv ... <schema_name>/<table_name>/LOAD0000000F.csv <schema_name>/<table_name>/LOAD00000010.csv ... In this lab we will: 1. Create IAM role needed for the rest of the labs. 2. Configure a Glue crawler accessing the raw data. 3. Define a few Glue jobs which will extract the data from the raw folder and save it to the destination folder in Apache Parquet format (partitioning will be done later). Create needed IAM Role \u00b6 Creating a Policy for Amazon S3 Bucket (Console) \u00b6 Sign in to the IAM console at https://console.aws.amazon.com/iam/ with your user that has administrator permissions. In the navigation pane, choose Policies. In the content pane, choose Create policy. Choose service as S3. Search for the following actions and mark them as checked: a. GetObject b. PutObject c. DeleteObject d. ListBucket In Resources section, click Add ARN for bucket and object resources, enter the name of your bucket, choose Any for all objects in the bucket or enter name of specific folder. When you are finished, choose Review policy Enter the name of policy as \u201cBYOD-S3Policy\u201d Creating a Role for AWS Service Glue (Console) \u00b6 Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane of the IAM console, choose Roles, and then choose Create role. For Select type of trusted entity, choose AWS service. Choose Glue as the service that you want to allow to assume this role. Choose Next: Permissions. Mark \u201cAWSGlueServiceRole\u201d policy as checked to attach to the role. Mark \u201cBYOD-S3Policy\u201d policy as checked to attach to the role. Choose Next: Tags. (Optional) Add metadata to the role by attaching tags as key\u2013value pairs. For more information about using tags in IAM, see Tagging IAM Users and Roles. Choose Next: Review. For Role name, enter \u201cglue-processor-role\u201d. (Optional) For Role description, type a description for the new role. Review the role and then choose Create role. NOTE: \u201cAWSGlueServiceRole\u201d is an AWS Managed Policy to provide Glue with needed permissions to access S3 data. However, you still need to allow access to your specific S3 bucket for Glue by attaching \u201cBYOD-S3Policy\u201d created policy. Add a crawler \u00b6 A crawler connects to a data store to determine the schema for your data, and then creates metadata tables in the data catalog. start by navigating to the Crawlers menu on the navigation pane, then press Add crawler . specify the name: {choose-name}-ds and press Next ; choose Data stores as Crawler source type and press Next ; Choose S3 as data store. Add S3 path where your raw data resides and press * Next ; At this stage we don\u2019t add any other data source; Choose the glue-processor-role as IAM Role and proceed to the schedule; Leave the Run on demand option at the Frequency section and press Next ; Click on the Add database button and specify {choose-name}_src as database name (this will be the name representing the source database in the data catalog). Press Next and Finish ; select the newly created crawler and push the Run crawler button. It will take a few minutes until it populates the data catalog. Schema Validation \u00b6 In the AWS Glue navigation pane, click Databases > Tables. (You can also click the database name (e.g., \u201cticketdata\u201d to browse the tables.). Within the Tables section of your database, click one table. You may notice that some tables have column headers such as col0,col1,col2,col3. In absence of headers or when the crawler cannot determine the header type, default column headers are specified. This exercise shows an example of how to resolve this issue. Click Edit Schema on the top right side. In the Edit Schema section, double-click col0 (column name) to open edit mode. Type a chosen name, e.g. \u201cid\u201d as the column name. Repeat the preceding step to change the remaining column names to match those shown in the following figure. NOTE: If you have any \u201cid\u201d column as integer, please make sure type is set to \u201cdouble\u201d. Click Save. Create the aggregation jobs \u00b6 In the following section, we will create some aggregations, which will serve as a primary data source for analysts. We place this data under the folder named \u201c curated \u201d in the data lake. In the Glue Console select the Jobs section in the left navigation panel\u2019 Click on the Add job button; specify a name in the name field, than select the \u201cglue-processor-role\u201d ; select the option \u201c A proposed script generated by AWS Glue \u201d; Tick the checkbox for \u201c Job Metrics \u201d, under Monitoring Options and hit Next ; select one table of the generated tables from your crawler and click Next; (You will need to repeat this for each table) On the Choose a transformation type page, select change schema On the Choose your data targets page, select Create tables in your data target. For Data store, select Amazon S3. For Format, select Parquet. For Target path, create a folder with name \u201ccurated\u201d in your bucket, create another folder inside curated with the table name, and choose it as a location to store the results e.g., \u201cs3://{YOUR_DATA_LAKE_BUCKET}/curated/{TABLE_NAME}\u201d Click Next. You can edit schema mapping to destination in this step. If you have an id column as integer type, please make sure it\u2019s changed to double by clicking Data type col. click Save job and edit script. NOTE: You will be re-visiting this step at the end of the labs to edit generated script and do partitioning for your data. This will show you how your Athena queries will perform better after partitioning. Currently running jobs multiple times will result in duplicate files being created in destination folders, which can give wrong results later with your queries. We will handle this in the partitioning section later. In the mean time, make sure your destination folders are empty each time if you want to run your jobs. We will run all jobs as a pipeline in the next lab.","title":"Transformation"},{"location":"01_ingestion_with_glue/ingestion_with_glue.html#introduction","text":"In this Lab we will create a schema from your data optimized for analytics and place the result in an S3 bucket based data lake.","title":"Introduction"},{"location":"01_ingestion_with_glue/ingestion_with_glue.html#before-you-begin","text":"Please make sure now you selected the region where your data resides. All resources to be created must to be in the same region.","title":"Before you begin"},{"location":"01_ingestion_with_glue/ingestion_with_glue.html#preparing-your-environment","text":"Before you start, make sure your raw data files are saved in a separate bucket in a folder called \u201craw\u201d. Each file should be a separate table. Each table file should be preferably in a separate folder with the table name. An example would be as follows: <schema_name>/<table_name>/LOAD00000001.csv <schema_name>/<table_name>/LOAD00000002.csv ... <schema_name>/<table_name>/LOAD00000009.csv <schema_name>/<table_name>/LOAD0000000A.csv <schema_name>/<table_name>/LOAD0000000B.csv ... <schema_name>/<table_name>/LOAD0000000F.csv <schema_name>/<table_name>/LOAD00000010.csv ... In this lab we will: 1. Create IAM role needed for the rest of the labs. 2. Configure a Glue crawler accessing the raw data. 3. Define a few Glue jobs which will extract the data from the raw folder and save it to the destination folder in Apache Parquet format (partitioning will be done later).","title":"Preparing your environment"},{"location":"01_ingestion_with_glue/ingestion_with_glue.html#create-needed-iam-role","text":"","title":"Create needed IAM Role"},{"location":"01_ingestion_with_glue/ingestion_with_glue.html#creating-a-policy-for-amazon-s3-bucket-console","text":"Sign in to the IAM console at https://console.aws.amazon.com/iam/ with your user that has administrator permissions. In the navigation pane, choose Policies. In the content pane, choose Create policy. Choose service as S3. Search for the following actions and mark them as checked: a. GetObject b. PutObject c. DeleteObject d. ListBucket In Resources section, click Add ARN for bucket and object resources, enter the name of your bucket, choose Any for all objects in the bucket or enter name of specific folder. When you are finished, choose Review policy Enter the name of policy as \u201cBYOD-S3Policy\u201d","title":"Creating a Policy for Amazon S3 Bucket (Console)"},{"location":"01_ingestion_with_glue/ingestion_with_glue.html#creating-a-role-for-aws-service-glue-console","text":"Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane of the IAM console, choose Roles, and then choose Create role. For Select type of trusted entity, choose AWS service. Choose Glue as the service that you want to allow to assume this role. Choose Next: Permissions. Mark \u201cAWSGlueServiceRole\u201d policy as checked to attach to the role. Mark \u201cBYOD-S3Policy\u201d policy as checked to attach to the role. Choose Next: Tags. (Optional) Add metadata to the role by attaching tags as key\u2013value pairs. For more information about using tags in IAM, see Tagging IAM Users and Roles. Choose Next: Review. For Role name, enter \u201cglue-processor-role\u201d. (Optional) For Role description, type a description for the new role. Review the role and then choose Create role. NOTE: \u201cAWSGlueServiceRole\u201d is an AWS Managed Policy to provide Glue with needed permissions to access S3 data. However, you still need to allow access to your specific S3 bucket for Glue by attaching \u201cBYOD-S3Policy\u201d created policy.","title":"Creating a Role for AWS Service Glue (Console)"},{"location":"01_ingestion_with_glue/ingestion_with_glue.html#add-a-crawler","text":"A crawler connects to a data store to determine the schema for your data, and then creates metadata tables in the data catalog. start by navigating to the Crawlers menu on the navigation pane, then press Add crawler . specify the name: {choose-name}-ds and press Next ; choose Data stores as Crawler source type and press Next ; Choose S3 as data store. Add S3 path where your raw data resides and press * Next ; At this stage we don\u2019t add any other data source; Choose the glue-processor-role as IAM Role and proceed to the schedule; Leave the Run on demand option at the Frequency section and press Next ; Click on the Add database button and specify {choose-name}_src as database name (this will be the name representing the source database in the data catalog). Press Next and Finish ; select the newly created crawler and push the Run crawler button. It will take a few minutes until it populates the data catalog.","title":"Add a crawler"},{"location":"01_ingestion_with_glue/ingestion_with_glue.html#schema-validation","text":"In the AWS Glue navigation pane, click Databases > Tables. (You can also click the database name (e.g., \u201cticketdata\u201d to browse the tables.). Within the Tables section of your database, click one table. You may notice that some tables have column headers such as col0,col1,col2,col3. In absence of headers or when the crawler cannot determine the header type, default column headers are specified. This exercise shows an example of how to resolve this issue. Click Edit Schema on the top right side. In the Edit Schema section, double-click col0 (column name) to open edit mode. Type a chosen name, e.g. \u201cid\u201d as the column name. Repeat the preceding step to change the remaining column names to match those shown in the following figure. NOTE: If you have any \u201cid\u201d column as integer, please make sure type is set to \u201cdouble\u201d. Click Save.","title":"Schema Validation"},{"location":"01_ingestion_with_glue/ingestion_with_glue.html#create-the-aggregation-jobs","text":"In the following section, we will create some aggregations, which will serve as a primary data source for analysts. We place this data under the folder named \u201c curated \u201d in the data lake. In the Glue Console select the Jobs section in the left navigation panel\u2019 Click on the Add job button; specify a name in the name field, than select the \u201cglue-processor-role\u201d ; select the option \u201c A proposed script generated by AWS Glue \u201d; Tick the checkbox for \u201c Job Metrics \u201d, under Monitoring Options and hit Next ; select one table of the generated tables from your crawler and click Next; (You will need to repeat this for each table) On the Choose a transformation type page, select change schema On the Choose your data targets page, select Create tables in your data target. For Data store, select Amazon S3. For Format, select Parquet. For Target path, create a folder with name \u201ccurated\u201d in your bucket, create another folder inside curated with the table name, and choose it as a location to store the results e.g., \u201cs3://{YOUR_DATA_LAKE_BUCKET}/curated/{TABLE_NAME}\u201d Click Next. You can edit schema mapping to destination in this step. If you have an id column as integer type, please make sure it\u2019s changed to double by clicking Data type col. click Save job and edit script. NOTE: You will be re-visiting this step at the end of the labs to edit generated script and do partitioning for your data. This will show you how your Athena queries will perform better after partitioning. Currently running jobs multiple times will result in duplicate files being created in destination folders, which can give wrong results later with your queries. We will handle this in the partitioning section later. In the mean time, make sure your destination folders are empty each time if you want to run your jobs. We will run all jobs as a pipeline in the next lab.","title":"Create the aggregation jobs"},{"location":"02_orchestration/orchestration.html","text":"Orchestrating the data pipline \u00b6 In this lab we will continue to work with Glue and convert the raw data we have extracted in the previous lab into a curated data set by making some aggregation. Please note : this Lab depends on the steps executed in the Transformation LAB; In case you didn\u2019t finish that one yet, now is the time :) At this stage we have Glue Jobs configured to covert our raw data into Parquet. We should be able to observe 2 main folders in our data-lake bucket (raw and curated). So far so good, however we\u2019d probably like to automate this process and repeat it on a daily basis for example. Orchestrate the data pipeline using the Workflow feature \u00b6 The plan is to create a pipeline, which will refresh the data-set every once in a while. The next steps assume you need to perform some jobs before running the others. However, if your transformation jobs can run in parallel, feel free to add them in parallel in the pipeline. An example of dependent jobs would be if you have jobs that extracts data from other source and ingest it into your data lake before kicking off another transformation jobs. Let\u2019s navigate to the Workflows in the ETL section on the left side-pane. we start by clicking on the Add Workflow button; add a name for our workflow (e.g. sportstickets-wf ) and press the Add Workflow button; Once the workflow is created add the first trigger to it. make sure you selected the Add New tab; define a Name for the new trigger ( transform-data ); specify the Frequency before you press Add (let\u2019s say you run this workflow at the 30th minute of every hour); Start by clicking on the newly created trigger, then add the needed jobs to it (that will run in parallel). Adding dependent jobs (Optional) \u00b6 Next we add one more trigger to start the aggregation as soon the ingestion is finished. Let\u2019s start by clicking on the Add trigger option in the top right corner of the workflow editor. Make sure that you select the Add new tab and Event as a Trigger type . Let\u2019s call our trigger transform . Don\u2019t save it just yet: Scroll down and select the \u201cStart after ALL watched event\u201d option at the Trigger Logic . This will make sure that we will only trigger the aggregation job, once all the ingestion jobs are finished. Now we have a trigger, we just need 2 more steps: add the dependency jobs (the ones to be watched) add the job which needs to be triggered once all watched ones are completed; Select the jobs that need to run first, make sure that we\u2019re watching for the SUCCEEDED event and push Add . We are almost there, however there\u2019s one more thing: the data we just generated is not available in the data catalog for wider audiences. Let\u2019s configure a one more crawler for our curated data and add it to the workflow. Register the staged data tables in the data catalog \u00b6 Navigate to the Crawlers section in the left pane and press Add crawler . at the name specify: {choose-name}_curated ; at the source types leave the default Data stores option; in the data-store section leave the default S3 and the Specified path in my account . now select your curated folder: s3://datalake-xxxxyyyyzzzzz/curated at the Choose an IAM role section choose the glue-processor-role and hit Next ; frequency stays On demand ; and finally we add a new database called {choose-name}_curated We return to the Workflows in the left side pane, select our created workflow and add one more trigger from the Action menu on the right. Select the Add new tab; Define the name as \u201c crawlers \u201d; Choose Event as Trigger type ; Select \u201c Start after ALL watched event \u201d and click Add ; Now select the newly created trigger in the workflow editor and click on the Add jobs/crawlers to watch . Select your jobs and hit Add . Select the Add jobs/crawlers to trigger option; This time select the Crawlers tab and check your curated crawler. Reviewing the results \u00b6 NOTE: In your created workflow, it starts by executing the jobs. However, because there is always new data being added or updated in your raw folder, you should later add your created crawler in the very beginning as the trigger to your transformation jobs. Once we are ready, we can try out the workflow by clicking on Run in the Actions menu. Once you selected a job, you can monitor it the execution status in the History TAB in the bottom panel; If the job(s) succeeded, visit the Metrics TAB to see resource utilisation and data movement information; Also note that the jobs can take quite a bit of time to end, about 15 minutes in total. When jobs succeed you should find in your S3 bucket a folder called curated with subfolders for all your tables. By selecting the latest Job and clicking on the View run details you can monitor the execution results of the data processing pipeline: Once the pipeline succeeded at least once, we should be able to observe the newly created databases in the data catalog. When you navigate to the Tables you will observe tables created from your data files. \u201cWhat can I do with it?\u201d, you may wonder. Stay tuned, we will cover this in great details in the next session.","title":"Orchestration"},{"location":"02_orchestration/orchestration.html#orchestrating-the-data-pipline","text":"In this lab we will continue to work with Glue and convert the raw data we have extracted in the previous lab into a curated data set by making some aggregation. Please note : this Lab depends on the steps executed in the Transformation LAB; In case you didn\u2019t finish that one yet, now is the time :) At this stage we have Glue Jobs configured to covert our raw data into Parquet. We should be able to observe 2 main folders in our data-lake bucket (raw and curated). So far so good, however we\u2019d probably like to automate this process and repeat it on a daily basis for example.","title":"Orchestrating the data pipline"},{"location":"02_orchestration/orchestration.html#orchestrate-the-data-pipeline-using-the-workflow-feature","text":"The plan is to create a pipeline, which will refresh the data-set every once in a while. The next steps assume you need to perform some jobs before running the others. However, if your transformation jobs can run in parallel, feel free to add them in parallel in the pipeline. An example of dependent jobs would be if you have jobs that extracts data from other source and ingest it into your data lake before kicking off another transformation jobs. Let\u2019s navigate to the Workflows in the ETL section on the left side-pane. we start by clicking on the Add Workflow button; add a name for our workflow (e.g. sportstickets-wf ) and press the Add Workflow button; Once the workflow is created add the first trigger to it. make sure you selected the Add New tab; define a Name for the new trigger ( transform-data ); specify the Frequency before you press Add (let\u2019s say you run this workflow at the 30th minute of every hour); Start by clicking on the newly created trigger, then add the needed jobs to it (that will run in parallel).","title":"Orchestrate the data pipeline using the Workflow feature"},{"location":"02_orchestration/orchestration.html#adding-dependent-jobs-optional","text":"Next we add one more trigger to start the aggregation as soon the ingestion is finished. Let\u2019s start by clicking on the Add trigger option in the top right corner of the workflow editor. Make sure that you select the Add new tab and Event as a Trigger type . Let\u2019s call our trigger transform . Don\u2019t save it just yet: Scroll down and select the \u201cStart after ALL watched event\u201d option at the Trigger Logic . This will make sure that we will only trigger the aggregation job, once all the ingestion jobs are finished. Now we have a trigger, we just need 2 more steps: add the dependency jobs (the ones to be watched) add the job which needs to be triggered once all watched ones are completed; Select the jobs that need to run first, make sure that we\u2019re watching for the SUCCEEDED event and push Add . We are almost there, however there\u2019s one more thing: the data we just generated is not available in the data catalog for wider audiences. Let\u2019s configure a one more crawler for our curated data and add it to the workflow.","title":"Adding dependent jobs (Optional)"},{"location":"02_orchestration/orchestration.html#register-the-staged-data-tables-in-the-data-catalog","text":"Navigate to the Crawlers section in the left pane and press Add crawler . at the name specify: {choose-name}_curated ; at the source types leave the default Data stores option; in the data-store section leave the default S3 and the Specified path in my account . now select your curated folder: s3://datalake-xxxxyyyyzzzzz/curated at the Choose an IAM role section choose the glue-processor-role and hit Next ; frequency stays On demand ; and finally we add a new database called {choose-name}_curated We return to the Workflows in the left side pane, select our created workflow and add one more trigger from the Action menu on the right. Select the Add new tab; Define the name as \u201c crawlers \u201d; Choose Event as Trigger type ; Select \u201c Start after ALL watched event \u201d and click Add ; Now select the newly created trigger in the workflow editor and click on the Add jobs/crawlers to watch . Select your jobs and hit Add . Select the Add jobs/crawlers to trigger option; This time select the Crawlers tab and check your curated crawler.","title":"Register the staged data tables in the data catalog"},{"location":"02_orchestration/orchestration.html#reviewing-the-results","text":"NOTE: In your created workflow, it starts by executing the jobs. However, because there is always new data being added or updated in your raw folder, you should later add your created crawler in the very beginning as the trigger to your transformation jobs. Once we are ready, we can try out the workflow by clicking on Run in the Actions menu. Once you selected a job, you can monitor it the execution status in the History TAB in the bottom panel; If the job(s) succeeded, visit the Metrics TAB to see resource utilisation and data movement information; Also note that the jobs can take quite a bit of time to end, about 15 minutes in total. When jobs succeed you should find in your S3 bucket a folder called curated with subfolders for all your tables. By selecting the latest Job and clicking on the View run details you can monitor the execution results of the data processing pipeline: Once the pipeline succeeded at least once, we should be able to observe the newly created databases in the data catalog. When you navigate to the Tables you will observe tables created from your data files. \u201cWhat can I do with it?\u201d, you may wonder. Stay tuned, we will cover this in great details in the next session.","title":"Reviewing the results"},{"location":"03_visualization_and_reporting/README.html","text":"Lab 2: Visualization using Amazon QuickSight \u00b6 Create an Amazon S3 bucket Creating Amazon Athena Database and Table Create Athena Database Create Athena Table Signing up for Amazon Quicksight Standard Edition Configuring Amazon QuickSight to use Amazon Athena as data source Visualizing the data using Amazon QuickSight Add year based filter to visualize the dataset for the year 2016 Add the month based filter for the month of January Visualize the data by hour of day for the month of January 2016 Visualize the data for the month of January 2016 for all taxi types(yellow, green, fhv) Architectural Diagram \u00b6 Create an Amazon S3 bucket \u00b6 Note: If you have already have an S3 bucket in your AWS Account you can skip this section. Open the AWS Management console for Amazon S3 On the S3 Dashboard, Click on Create Bucket . In the Create Bucket pop-up page, input a unique Bucket name . It is advised to choose a large bucket name, with many random characters and numbers (no spaces). Select the region as Oregon . Click Next to navigate to next tab. In the Set properties tab, leave all options as default. In the Set permissions tag, leave all options as default. In the Review tab, click on Create Bucket Creating Amazon Athena Database and Table \u00b6 Note: If you have complete the Lab 1: Serverless Analysis of data in Amazon S3 using Amazon Athena you can skip this section and go to the next section Signing up for Amazon Quicksight Standard Edition Amazon Athena uses Apache Hive to define tables and create databases. Databases are a logical grouping of tables. When you create a database and table in Athena, you are simply describing the schema and location of the table data in Amazon S3. In case of Hive, databases and tables don\u2019t store the data along with the schema definition unlike traditional relational database systems. The data is read from Amazon S3 only when you query the table. The other benefit of using Hive is that the metastore found in Hive can be used in many other big data applications such as Spark, Hadoop, and Presto. With Athena catalog, you can now have Hive-compatible metastore in the cloud without the need for provisioning a Hadoop cluster or RDS instance. For guidance on databases and tables creation refer Apache Hive documentation . The following steps provides guidance specifically for Amazon Athena. Setting up Athena (first time users) \u00b6 If you\u2019re a first time Athena user, you might need to configure an S3 bucket, where Athena will store the query results. You can use an already existing bucket with a dedicated folder or you can create a new, dedicated bucket. NOTE: Make sure you have forward slash at the end of the S3 path Create Database \u00b6 Open the AWS Management Console for Athena . If this is your first time visiting the AWS Management Console for Athena, you will get a Getting Started page. Choose Get Started to open the Query Editor. If this isn\u2019t your first time, the Athena Query Editor opens. Make a note of the AWS region name, for example, for this lab you will need to choose the US West (Oregon) region. In the Athena Query Editor , you will see a query pane with an example query. Now you can start entering your query in the query pane. To create a database named mydatabase , copy the following statement, and then choose Run Query : CREATE DATABASE mydatabase Ensure mydatabase appears in the DATABASE list on the Catalog dashboard Create a Table \u00b6 Ensure that current AWS region is US West (Oregon) region Ensure mydatabase is selected from the DATABASE list and then choose New Query . In the query pane, copy the following statement to create a the NYTaxiRides table, and then choose Run Query : CREATE EXTERNAL TABLE NYTaxiRides ( vendorid STRING, pickup_datetime TIMESTAMP, dropoff_datetime TIMESTAMP, ratecode INT, passenger_count INT, trip_distance DOUBLE, fare_amount DOUBLE, total_amount DOUBLE, payment_type INT ) PARTITIONED BY (YEAR INT, MONTH INT, TYPE string) STORED AS PARQUET LOCATION 's3://us-west-2.serverless-analytics/canonical/NY-Pub' 4.Ensure the table you just created appears on the Catalog dashboard for the selected database. Now that you have created the table you need to add the partition metadata to the Amazon Athena Catalog. Choose New Query , copy the following statement into the query pane, and then choose Run Query to add partition metadata. MSCK REPAIR TABLE NYTaxiRides The returned result will contain information for the partitions that are added to NYTaxiRides for each taxi type (yellow, green, fhv) for every month for the year from 2009 to 2016 Signing up for Amazon QuickSight Enterprise Edition \u00b6 Open the AWS Management Console for QuickSight . If this is the first time you are accessing QuickSight, you will see a sign-up landing page for QuickSight. Click on Sign up for QuickSight . Note: Chrome browser might timeout at this step. If that\u2019s the case, try this step in Firefox/Microsoft Edge/Safari. On the next page, for the subscription type select the \u201cEnterprise Edition\u201d and click Continue . On the next page, i. Enter a unique QuickSight account name. ii. Enter a valid email for Notification email address . iii. Just for this step, leave the QuickSight capacity region as N.Virginia . iv. Ensure that Enable autodiscovery of your data and users in your Amazon Redshift, Amazon RDS and AWS IAM Services and Amazon Athena boxes are checked. v. Click Finish . vi. You will be presented with a message Congratulations ! You are signed up for Amazon QuickSight! on successful sign up. Click on Go to Amazon QuickSight . Before continuing with the following steps, make sure you are in the N. Virginia Region to edit permissions. Now, on the Amazon QuickSight dashboard, navigate to User Settings page on the Top-Right section and click Manage QuickSight . In this section, click on Security & permissions and then click Add or remove . Click on Amazon S3 and on the tab that says S3 buckets linked to QuickSight account . Ensure Select All is checked. Click on Select buckets . When you are done doing all this, click Update to bring you back to the user settings back. Configuring Amazon QuickSight to use Amazon Athena as data source \u00b6 For this lab, you will need to choose the region where your data resides. Click on the region icon on the top-right corner of the page, and select the region where your data resides. Click on Manage data on the top-right corner of the webpage to review existing data sets. Click on New data set on the top-left corner of the webpage and review the options. Select Athena as a Data source. Enter the Data source name (e.g. AthenaDataSource ). Select the Athena workgroup you created specifically for Quicksight. Then Validate the Connection . Click Create data source . Choose the table you need to visualize its data. Choose Save and Visualize on top of the page. Alternative Option You can choose to create a dataset using S3 as your data source. For this: * Make sure you have granted Amazon QuickSight access to any Amazon S3 buckets that you want to read files from. * Create a manifest file to identify the text files that you want to import. Supported Formats for Amazon S3 Manifest Files Preparing your data \u00b6 You can edit an existing data set to perform data preparation. To edit a data set from the Your Data Sets page , choose the data set, and then choose Edit Data Set . The data set opens in the data preparation page. If you want to create a copy of the data set, choose Duplicate data set, and enter a name for the copy. Select the fields that you will use for the visualization. We suggest that you pick two - three columns from your data set that meet the following criteria: . The first column is a date column (can be year, month or day. Usually marked by calendar icon in Fields list on the left) . The second column is a quantifiable number (revenue, count, distance, etc. Usually marked by a green hash # ) *. The third column has categorical value, which means it has specific limited set of values (type, category, etc. Usually marked by ticket icon ) Optional - Change the data type. You can change the field\u2019s data type in one of the available data types. You can also modify the format of your date field(s) into one of the supported formats. Visualizing the data using Amazon QuickSight \u00b6 Now that you have configured the data source and prepared the dataset to work with, we will start by forecasting values in future dates based on your sample data. Forecast Monthly Trend for your Quantity Column \u00b6 Under the Fields list , Select your Date column for x-axis by clicking on the field name. Change the visual type to a line chart by selecting the line chart icon highlighted in the screenshot below under Visual types . At this point, the Y-axis of the visual will be populated automatically with count of records that match each date individually. You can keep it that way and do forecasting for count of records , or choose another quantity attribute from Fields list to populate Y-axis automatically and have more meaningful forecast. Before viewing the forecast, you can choose the level of aggregation you want for your date column to populate X-axis by year, month or day. 4. Click on the date field name in top Field Wells bar to reveal a sub-menu. 5. Select Aggregate:Month to aggregate by month. You can also use the slider on the X-axis to select the range of values to appear in the graph. Click arrow in top right corner of the visual and select Add forecast . NOTE: Make sure your Y-axis is assigned to a quantity column before proceeding. Adding Filters \u00b6 You can apply filters to both regular and calculated fields, which include text, numeric, and date fields. Let\u2019s apply a date filter: Choose Filter on the tool bar. On the Applied filters pane, choose Create one , and then choose a date field to filter on. Choose in which visual the filter will apply and choose the filter type from the dropdown list. Choose a comparison type. Enter date values. Choose Apply. Visualize Month over Month Quantity \u00b6 Add a new visual by duplicating the previous visual. Click on visual top right arrow and select Duplicate visual . Select KPI as the Visual Type (bottom left of the screen). In the field wells, click arrow in Date column to change the aggregation level to Month or as needed. Now select format visual by clicking on arrow on top right corner of the KPI graph. Select Different as percent(%) under comparison method on the left. Review ML Insights \u00b6 Click the \u2018Insights\u2019 menu on the left. Notice all the suggested insights QuickSight has generated based on what has been built so far! Hover over any of the insights and click the \u2018+\u2019 to add it to the dashboard. NOTE: You can customize the narrative by clicking on top right arrow of the visual and selecting Customize narrative . Note: The interesting outlier in the above graph is that on Jan23rd, 2016, you see the dip in the number of taxis across all types. Doing a quick google search for that date, gets us this weather article from NBC New York Using Amazon QuickSight, you were able to see patterns across a time-series data by building visualizations, performing ad-hoc analysis, and quickly generating insights. License \u00b6 This library is licensed under the Apache 2.0 License.","title":"Visualization and Reporting"},{"location":"03_visualization_and_reporting/README.html#lab-2-visualization-using-amazon-quicksight","text":"Create an Amazon S3 bucket Creating Amazon Athena Database and Table Create Athena Database Create Athena Table Signing up for Amazon Quicksight Standard Edition Configuring Amazon QuickSight to use Amazon Athena as data source Visualizing the data using Amazon QuickSight Add year based filter to visualize the dataset for the year 2016 Add the month based filter for the month of January Visualize the data by hour of day for the month of January 2016 Visualize the data for the month of January 2016 for all taxi types(yellow, green, fhv)","title":"Lab 2: Visualization using Amazon QuickSight"},{"location":"03_visualization_and_reporting/README.html#architectural-diagram","text":"","title":"Architectural Diagram"},{"location":"03_visualization_and_reporting/README.html#create-an-amazon-s3-bucket","text":"Note: If you have already have an S3 bucket in your AWS Account you can skip this section. Open the AWS Management console for Amazon S3 On the S3 Dashboard, Click on Create Bucket . In the Create Bucket pop-up page, input a unique Bucket name . It is advised to choose a large bucket name, with many random characters and numbers (no spaces). Select the region as Oregon . Click Next to navigate to next tab. In the Set properties tab, leave all options as default. In the Set permissions tag, leave all options as default. In the Review tab, click on Create Bucket","title":"Create an Amazon S3 bucket"},{"location":"03_visualization_and_reporting/README.html#creating-amazon-athena-database-and-table","text":"Note: If you have complete the Lab 1: Serverless Analysis of data in Amazon S3 using Amazon Athena you can skip this section and go to the next section Signing up for Amazon Quicksight Standard Edition Amazon Athena uses Apache Hive to define tables and create databases. Databases are a logical grouping of tables. When you create a database and table in Athena, you are simply describing the schema and location of the table data in Amazon S3. In case of Hive, databases and tables don\u2019t store the data along with the schema definition unlike traditional relational database systems. The data is read from Amazon S3 only when you query the table. The other benefit of using Hive is that the metastore found in Hive can be used in many other big data applications such as Spark, Hadoop, and Presto. With Athena catalog, you can now have Hive-compatible metastore in the cloud without the need for provisioning a Hadoop cluster or RDS instance. For guidance on databases and tables creation refer Apache Hive documentation . The following steps provides guidance specifically for Amazon Athena.","title":"Creating Amazon Athena Database and Table"},{"location":"03_visualization_and_reporting/README.html#setting-up-athena-first-time-users","text":"If you\u2019re a first time Athena user, you might need to configure an S3 bucket, where Athena will store the query results. You can use an already existing bucket with a dedicated folder or you can create a new, dedicated bucket. NOTE: Make sure you have forward slash at the end of the S3 path","title":"Setting up Athena (first time users)"},{"location":"03_visualization_and_reporting/README.html#create-database","text":"Open the AWS Management Console for Athena . If this is your first time visiting the AWS Management Console for Athena, you will get a Getting Started page. Choose Get Started to open the Query Editor. If this isn\u2019t your first time, the Athena Query Editor opens. Make a note of the AWS region name, for example, for this lab you will need to choose the US West (Oregon) region. In the Athena Query Editor , you will see a query pane with an example query. Now you can start entering your query in the query pane. To create a database named mydatabase , copy the following statement, and then choose Run Query : CREATE DATABASE mydatabase Ensure mydatabase appears in the DATABASE list on the Catalog dashboard","title":"Create Database"},{"location":"03_visualization_and_reporting/README.html#create-a-table","text":"Ensure that current AWS region is US West (Oregon) region Ensure mydatabase is selected from the DATABASE list and then choose New Query . In the query pane, copy the following statement to create a the NYTaxiRides table, and then choose Run Query : CREATE EXTERNAL TABLE NYTaxiRides ( vendorid STRING, pickup_datetime TIMESTAMP, dropoff_datetime TIMESTAMP, ratecode INT, passenger_count INT, trip_distance DOUBLE, fare_amount DOUBLE, total_amount DOUBLE, payment_type INT ) PARTITIONED BY (YEAR INT, MONTH INT, TYPE string) STORED AS PARQUET LOCATION 's3://us-west-2.serverless-analytics/canonical/NY-Pub' 4.Ensure the table you just created appears on the Catalog dashboard for the selected database. Now that you have created the table you need to add the partition metadata to the Amazon Athena Catalog. Choose New Query , copy the following statement into the query pane, and then choose Run Query to add partition metadata. MSCK REPAIR TABLE NYTaxiRides The returned result will contain information for the partitions that are added to NYTaxiRides for each taxi type (yellow, green, fhv) for every month for the year from 2009 to 2016","title":"Create a Table"},{"location":"03_visualization_and_reporting/README.html#signing-up-for-amazon-quicksight-enterprise-edition","text":"Open the AWS Management Console for QuickSight . If this is the first time you are accessing QuickSight, you will see a sign-up landing page for QuickSight. Click on Sign up for QuickSight . Note: Chrome browser might timeout at this step. If that\u2019s the case, try this step in Firefox/Microsoft Edge/Safari. On the next page, for the subscription type select the \u201cEnterprise Edition\u201d and click Continue . On the next page, i. Enter a unique QuickSight account name. ii. Enter a valid email for Notification email address . iii. Just for this step, leave the QuickSight capacity region as N.Virginia . iv. Ensure that Enable autodiscovery of your data and users in your Amazon Redshift, Amazon RDS and AWS IAM Services and Amazon Athena boxes are checked. v. Click Finish . vi. You will be presented with a message Congratulations ! You are signed up for Amazon QuickSight! on successful sign up. Click on Go to Amazon QuickSight . Before continuing with the following steps, make sure you are in the N. Virginia Region to edit permissions. Now, on the Amazon QuickSight dashboard, navigate to User Settings page on the Top-Right section and click Manage QuickSight . In this section, click on Security & permissions and then click Add or remove . Click on Amazon S3 and on the tab that says S3 buckets linked to QuickSight account . Ensure Select All is checked. Click on Select buckets . When you are done doing all this, click Update to bring you back to the user settings back.","title":"Signing up for Amazon QuickSight Enterprise Edition"},{"location":"03_visualization_and_reporting/README.html#configuring-amazon-quicksight-to-use-amazon-athena-as-data-source","text":"For this lab, you will need to choose the region where your data resides. Click on the region icon on the top-right corner of the page, and select the region where your data resides. Click on Manage data on the top-right corner of the webpage to review existing data sets. Click on New data set on the top-left corner of the webpage and review the options. Select Athena as a Data source. Enter the Data source name (e.g. AthenaDataSource ). Select the Athena workgroup you created specifically for Quicksight. Then Validate the Connection . Click Create data source . Choose the table you need to visualize its data. Choose Save and Visualize on top of the page. Alternative Option You can choose to create a dataset using S3 as your data source. For this: * Make sure you have granted Amazon QuickSight access to any Amazon S3 buckets that you want to read files from. * Create a manifest file to identify the text files that you want to import. Supported Formats for Amazon S3 Manifest Files","title":"Configuring Amazon QuickSight to use Amazon Athena as data source"},{"location":"03_visualization_and_reporting/README.html#preparing-your-data","text":"You can edit an existing data set to perform data preparation. To edit a data set from the Your Data Sets page , choose the data set, and then choose Edit Data Set . The data set opens in the data preparation page. If you want to create a copy of the data set, choose Duplicate data set, and enter a name for the copy. Select the fields that you will use for the visualization. We suggest that you pick two - three columns from your data set that meet the following criteria: . The first column is a date column (can be year, month or day. Usually marked by calendar icon in Fields list on the left) . The second column is a quantifiable number (revenue, count, distance, etc. Usually marked by a green hash # ) *. The third column has categorical value, which means it has specific limited set of values (type, category, etc. Usually marked by ticket icon ) Optional - Change the data type. You can change the field\u2019s data type in one of the available data types. You can also modify the format of your date field(s) into one of the supported formats.","title":"Preparing your data"},{"location":"03_visualization_and_reporting/README.html#visualizing-the-data-using-amazon-quicksight","text":"Now that you have configured the data source and prepared the dataset to work with, we will start by forecasting values in future dates based on your sample data.","title":"Visualizing the data using Amazon QuickSight"},{"location":"03_visualization_and_reporting/README.html#forecast-monthly-trend-for-your-quantity-column","text":"Under the Fields list , Select your Date column for x-axis by clicking on the field name. Change the visual type to a line chart by selecting the line chart icon highlighted in the screenshot below under Visual types . At this point, the Y-axis of the visual will be populated automatically with count of records that match each date individually. You can keep it that way and do forecasting for count of records , or choose another quantity attribute from Fields list to populate Y-axis automatically and have more meaningful forecast. Before viewing the forecast, you can choose the level of aggregation you want for your date column to populate X-axis by year, month or day. 4. Click on the date field name in top Field Wells bar to reveal a sub-menu. 5. Select Aggregate:Month to aggregate by month. You can also use the slider on the X-axis to select the range of values to appear in the graph. Click arrow in top right corner of the visual and select Add forecast . NOTE: Make sure your Y-axis is assigned to a quantity column before proceeding.","title":"Forecast Monthly Trend for your Quantity Column"},{"location":"03_visualization_and_reporting/README.html#adding-filters","text":"You can apply filters to both regular and calculated fields, which include text, numeric, and date fields. Let\u2019s apply a date filter: Choose Filter on the tool bar. On the Applied filters pane, choose Create one , and then choose a date field to filter on. Choose in which visual the filter will apply and choose the filter type from the dropdown list. Choose a comparison type. Enter date values. Choose Apply.","title":"Adding Filters"},{"location":"03_visualization_and_reporting/README.html#visualize-month-over-month-quantity","text":"Add a new visual by duplicating the previous visual. Click on visual top right arrow and select Duplicate visual . Select KPI as the Visual Type (bottom left of the screen). In the field wells, click arrow in Date column to change the aggregation level to Month or as needed. Now select format visual by clicking on arrow on top right corner of the KPI graph. Select Different as percent(%) under comparison method on the left.","title":"Visualize Month over Month Quantity"},{"location":"03_visualization_and_reporting/README.html#review-ml-insights","text":"Click the \u2018Insights\u2019 menu on the left. Notice all the suggested insights QuickSight has generated based on what has been built so far! Hover over any of the insights and click the \u2018+\u2019 to add it to the dashboard. NOTE: You can customize the narrative by clicking on top right arrow of the visual and selecting Customize narrative . Note: The interesting outlier in the above graph is that on Jan23rd, 2016, you see the dip in the number of taxis across all types. Doing a quick google search for that date, gets us this weather article from NBC New York Using Amazon QuickSight, you were able to see patterns across a time-series data by building visualizations, performing ad-hoc analysis, and quickly generating insights.","title":"Review ML Insights"},{"location":"03_visualization_and_reporting/README.html#license","text":"This library is licensed under the Apache 2.0 License.","title":"License"}]}